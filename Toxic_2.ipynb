{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Data preparation for Deep learning Model:\n"
      ],
      "metadata": {
        "id": "XPqg42Ubrh2W"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "4F1emwOLrZmQ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import PIL\n",
        "import sys\n",
        "\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.models import Sequential\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mounting the cleaned data from the google drive:"
      ],
      "metadata": {
        "id": "ISR2RxwmuC9Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path = '/content/drive/MyDrive/Colab Notebooks/clean_df.csv'\n",
        "df = pd.read_csv(path)"
      ],
      "metadata": {
        "id": "ewumfgtyuM42"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "PyuVT4ivussk",
        "outputId": "d79cc4a1-c287-40d4-fde2-4d2e7fa0652c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                            comment_text  toxic  severe_toxic  \\\n",
              "0                                     fuck wolfkeep talk      1             1   \n",
              "1      user talk png crusad bot hate mindless bot min...      0             0   \n",
              "2      probabl anoth station name ridgewood go check one      0             0   \n",
              "3      next time happen report eat children rarghhhh ...      1             1   \n",
              "4      new accord japanes ladi name kama chinen recen...      0             0   \n",
              "...                                                  ...    ...           ...   \n",
              "31220  correctionperson correct vandal blood libel ac...      1             0   \n",
              "31221  septemb hello welcom wikipedia hope seem unfri...      0             0   \n",
              "31222  welcom hello welcom wikipedia thank contribut ...      0             0   \n",
              "31223                                    fuck touch edit      1             0   \n",
              "31224  prior quickpol would perma block guess remov b...      1             0   \n",
              "\n",
              "       obscene  threat  insult  identity_hate  \n",
              "0            1       0       1              0  \n",
              "1            0       0       0              0  \n",
              "2            0       0       0              0  \n",
              "3            1       1       1              0  \n",
              "4            0       0       0              0  \n",
              "...        ...     ...     ...            ...  \n",
              "31220        0       0       0              0  \n",
              "31221        0       0       0              0  \n",
              "31222        0       0       0              0  \n",
              "31223        1       0       0              0  \n",
              "31224        0       0       0              0  \n",
              "\n",
              "[31225 rows x 7 columns]"
            ],
            "text/html": [
              "\n",
              "\n",
              "  <div id=\"df-6719fb2c-5052-4bc3-a57d-e0c97b73da68\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>comment_text</th>\n",
              "      <th>toxic</th>\n",
              "      <th>severe_toxic</th>\n",
              "      <th>obscene</th>\n",
              "      <th>threat</th>\n",
              "      <th>insult</th>\n",
              "      <th>identity_hate</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>fuck wolfkeep talk</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>user talk png crusad bot hate mindless bot min...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>probabl anoth station name ridgewood go check one</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>next time happen report eat children rarghhhh ...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>new accord japanes ladi name kama chinen recen...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31220</th>\n",
              "      <td>correctionperson correct vandal blood libel ac...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31221</th>\n",
              "      <td>septemb hello welcom wikipedia hope seem unfri...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31222</th>\n",
              "      <td>welcom hello welcom wikipedia thank contribut ...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31223</th>\n",
              "      <td>fuck touch edit</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31224</th>\n",
              "      <td>prior quickpol would perma block guess remov b...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>31225 rows × 7 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6719fb2c-5052-4bc3-a57d-e0c97b73da68')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "\n",
              "\n",
              "\n",
              "    <div id=\"df-29578ada-3472-4ed7-973d-2a286c6a86b4\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-29578ada-3472-4ed7-973d-2a286c6a86b4')\"\n",
              "              title=\"Suggest charts.\"\n",
              "              style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "    </div>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "    background-color: #E8F0FE;\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: #1967D2;\n",
              "    height: 32px;\n",
              "    padding: 0 0 0 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: #E2EBFA;\n",
              "    box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: #174EA6;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "    background-color: #3B4455;\n",
              "    fill: #D2E3FC;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart:hover {\n",
              "    background-color: #434B5C;\n",
              "    box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "    filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "    fill: #FFFFFF;\n",
              "  }\n",
              "</style>\n",
              "\n",
              "    <script>\n",
              "      async function quickchart(key) {\n",
              "        const containerElement = document.querySelector('#' + key);\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      }\n",
              "    </script>\n",
              "\n",
              "      <script>\n",
              "\n",
              "function displayQuickchartButton(domScope) {\n",
              "  let quickchartButtonEl =\n",
              "    domScope.querySelector('#df-29578ada-3472-4ed7-973d-2a286c6a86b4 button.colab-df-quickchart');\n",
              "  quickchartButtonEl.style.display =\n",
              "    google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "}\n",
              "\n",
              "        displayQuickchartButton(document);\n",
              "      </script>\n",
              "      <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-6719fb2c-5052-4bc3-a57d-e0c97b73da68 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-6719fb2c-5052-4bc3-a57d-e0c97b73da68');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7U2WkorKusBx",
        "outputId": "8bf65d5b-3b22-4f4f-86db-23dc49685157"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(31225, 7)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QCIMjeX4vENz",
        "outputId": "475caa6f-39d7-4aa2-8695-53aabe7f367d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 31225 entries, 0 to 31224\n",
            "Data columns (total 7 columns):\n",
            " #   Column         Non-Null Count  Dtype \n",
            "---  ------         --------------  ----- \n",
            " 0   comment_text   31220 non-null  object\n",
            " 1   toxic          31225 non-null  int64 \n",
            " 2   severe_toxic   31225 non-null  int64 \n",
            " 3   obscene        31225 non-null  int64 \n",
            " 4   threat         31225 non-null  int64 \n",
            " 5   insult         31225 non-null  int64 \n",
            " 6   identity_hate  31225 non-null  int64 \n",
            "dtypes: int64(6), object(1)\n",
            "memory usage: 1.7+ MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Double checking for Empty/Nan comments after the Pre-cleaning and Pre-processing"
      ],
      "metadata": {
        "id": "-hX_oZmGOf6D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The results show there is Nan comments\n",
        "df.isnull().values.any()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wVSEfz3DM-pb",
        "outputId": "8bac80c7-aab2-457d-be21-ad8635becbc4"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "index=df[df['comment_text'].isnull()].index"
      ],
      "metadata": {
        "id": "nMzmJF3QM-oh"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(index)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M437yKZ0M-mI",
        "outputId": "8f696dd4-aa9a-41c5-dcb0-911c657446a1"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample=np.random.choice(index,5)\n",
        "\n",
        "for i in sample:\n",
        "    print(i,\"- index data after pre-processing-->\",df['comment_text'].values[i])\n",
        "    print(\"=\"*100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kyn9ROjUM-hs",
        "outputId": "ecf9ee69-26c9-44e4-9181-4bf13bc422be"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "15152 - index data after pre-processing--> nan\n",
            "====================================================================================================\n",
            "15152 - index data after pre-processing--> nan\n",
            "====================================================================================================\n",
            "4356 - index data after pre-processing--> nan\n",
            "====================================================================================================\n",
            "15152 - index data after pre-processing--> nan\n",
            "====================================================================================================\n",
            "14338 - index data after pre-processing--> nan\n",
            "====================================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Noticed that during pre-processing after all the unwanted data has been cleaned ,there was no data remains in certain rows of the  'comments_ text' columns. And it resulting in Nan value.\n",
        "\n",
        "Therefore need to clean the Data again to remove the empty values.\n"
      ],
      "metadata": {
        "id": "3B6YrN4VOZNU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Elimination of Nan/empty data\n",
        "df.dropna(inplace=True)"
      ],
      "metadata": {
        "id": "IOovclF0N2Kh"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.isnull().values.any()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PUZBFv36N2Az",
        "outputId": "e4f8e214-ff6c-46ba-84eb-71fce4b2561b"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9L_YimCbN19I",
        "outputId": "48547812-453d-4794-bfdb-6b914098e1ef"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(31220, 7)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZNkIedx8OQSh",
        "outputId": "20bbe779-7d64-4633-efa6-6fb01613adb4"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(31220, 7)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X=df['comment_text']\n",
        "y=df.drop(columns=['comment_text'])"
      ],
      "metadata": {
        "id": "iQgY9h1jOQOT"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Vectorization of the text data"
      ],
      "metadata": {
        "id": "Kov-YBB07LkL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import TextVectorization"
      ],
      "metadata": {
        "id": "QxoAoQlLaKal"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_word = 200000"
      ],
      "metadata": {
        "id": "1pSyJkIQQCFb"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Taking words and converting to intiger\n",
        "vectorizer = TextVectorization(max_tokens= max_word,output_sequence_length=1800,output_mode='int')"
      ],
      "metadata": {
        "id": "Li_FnqDgQB-2"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#For the machine too learn all the words in our data set\n",
        "vectorizer.adapt(X.values)"
      ],
      "metadata": {
        "id": "jI414CN7QB9Z"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer.get_vocabulary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xAtl1v49QB5h",
        "outputId": "0b1ebf20-2e08-46a7-946a-5f0f2730c515"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['',\n",
              " '[UNK]',\n",
              " 'fuck',\n",
              " 'articl',\n",
              " 'wikipedia',\n",
              " 'page',\n",
              " 'like',\n",
              " 'edit',\n",
              " 'talk',\n",
              " 'go',\n",
              " 'one',\n",
              " 'suck',\n",
              " 'use',\n",
              " 'know',\n",
              " 'get',\n",
              " 'would',\n",
              " 'pleas',\n",
              " 'delet',\n",
              " 'peopl',\n",
              " 'shit',\n",
              " 'think',\n",
              " 'make',\n",
              " 'user',\n",
              " 'block',\n",
              " 'nigger',\n",
              " 'see',\n",
              " 'time',\n",
              " 'u',\n",
              " 'thank',\n",
              " 'ass',\n",
              " 'sourc',\n",
              " 'say',\n",
              " 'want',\n",
              " 'also',\n",
              " 'hate',\n",
              " 'faggot',\n",
              " 'person',\n",
              " 'hi',\n",
              " 'need',\n",
              " 'even',\n",
              " 'vandal',\n",
              " 'remov',\n",
              " 'gay',\n",
              " 'die',\n",
              " 'bitch',\n",
              " 'look',\n",
              " 'name',\n",
              " 'wiki',\n",
              " 'stop',\n",
              " 'good',\n",
              " 'work',\n",
              " 'right',\n",
              " 'link',\n",
              " 'take',\n",
              " 'thing',\n",
              " 'well',\n",
              " 'tri',\n",
              " 'comment',\n",
              " 'way',\n",
              " 'moron',\n",
              " 'inform',\n",
              " 'fat',\n",
              " 'chang',\n",
              " 'may',\n",
              " 'discuss',\n",
              " 'cunt',\n",
              " 'read',\n",
              " 'call',\n",
              " 'realli',\n",
              " 'help',\n",
              " 'imag',\n",
              " 'jew',\n",
              " 'editor',\n",
              " 'admin',\n",
              " 'fact',\n",
              " 'revert',\n",
              " 'refer',\n",
              " 'come',\n",
              " 'could',\n",
              " 'back',\n",
              " 'stupid',\n",
              " 'new',\n",
              " 'ad',\n",
              " 'point',\n",
              " 'reason',\n",
              " 'post',\n",
              " 'dick',\n",
              " 'first',\n",
              " 'mean',\n",
              " 'much',\n",
              " 'seem',\n",
              " 'give',\n",
              " 'mani',\n",
              " 'made',\n",
              " 'life',\n",
              " 'section',\n",
              " 'state',\n",
              " 'question',\n",
              " 'list',\n",
              " 'find',\n",
              " 'still',\n",
              " 'pig',\n",
              " 'said',\n",
              " 'love',\n",
              " 'littl',\n",
              " 'wp',\n",
              " 'place',\n",
              " 'year',\n",
              " 'ban',\n",
              " 'ask',\n",
              " 'word',\n",
              " 'keep',\n",
              " 'attack',\n",
              " 'someon',\n",
              " 'idiot',\n",
              " 'care',\n",
              " 'day',\n",
              " 'cock',\n",
              " 'dont',\n",
              " 'put',\n",
              " 'someth',\n",
              " 'guy',\n",
              " 'sinc',\n",
              " 'peni',\n",
              " 'tell',\n",
              " 'contribut',\n",
              " 'actual',\n",
              " 'noth',\n",
              " 'let',\n",
              " 'never',\n",
              " 'hey',\n",
              " 'http',\n",
              " 'bullshit',\n",
              " 'asshol',\n",
              " 'us',\n",
              " 'show',\n",
              " 'two',\n",
              " 'feel',\n",
              " 'kill',\n",
              " 'bad',\n",
              " 'anyth',\n",
              " 'creat',\n",
              " 'claim',\n",
              " 'hope',\n",
              " 'site',\n",
              " 'rule',\n",
              " 'write',\n",
              " 'add',\n",
              " 'includ',\n",
              " 'old',\n",
              " 'histori',\n",
              " 'best',\n",
              " 'ball',\n",
              " 'fag',\n",
              " 'oh',\n",
              " 'leav',\n",
              " 'anoth',\n",
              " 'believ',\n",
              " 'wanker',\n",
              " 'sure',\n",
              " 'issu',\n",
              " 'world',\n",
              " 'consid',\n",
              " 'problem',\n",
              " 'understand',\n",
              " 'sex',\n",
              " 'better',\n",
              " 'without',\n",
              " 'bark',\n",
              " 'live',\n",
              " 'piec',\n",
              " 'wrong',\n",
              " 'eat',\n",
              " 'note',\n",
              " 'content',\n",
              " 'case',\n",
              " 'start',\n",
              " 'polici',\n",
              " 'done',\n",
              " 'utc',\n",
              " 'real',\n",
              " 'tag',\n",
              " 'continu',\n",
              " 'ever',\n",
              " 'differ',\n",
              " 'warn',\n",
              " 'must',\n",
              " 'free',\n",
              " 'war',\n",
              " 'anyon',\n",
              " 'support',\n",
              " 'mention',\n",
              " 'hell',\n",
              " 'messag',\n",
              " 'everi',\n",
              " 'suggest',\n",
              " 'account',\n",
              " 'got',\n",
              " 'view',\n",
              " 'man',\n",
              " 'might',\n",
              " 'part',\n",
              " 'long',\n",
              " 'com',\n",
              " 'last',\n",
              " 'big',\n",
              " 'ip',\n",
              " 'cocksuck',\n",
              " 'subject',\n",
              " 'opinion',\n",
              " 'aid',\n",
              " 'report',\n",
              " 'interest',\n",
              " 'nippl',\n",
              " 'f',\n",
              " 'bastard',\n",
              " 'alreadi',\n",
              " 'howev',\n",
              " 'other',\n",
              " 'administr',\n",
              " 'notabl',\n",
              " 'book',\n",
              " 'agre',\n",
              " 'follow',\n",
              " 'request',\n",
              " 'dickhead',\n",
              " 'fucker',\n",
              " 'damn',\n",
              " 'around',\n",
              " 'check',\n",
              " 'regard',\n",
              " 'rape',\n",
              " 'sorri',\n",
              " 'gener',\n",
              " 'shut',\n",
              " 'god',\n",
              " 'though',\n",
              " 'ignor',\n",
              " 'provid',\n",
              " 'origin',\n",
              " 'els',\n",
              " 'loser',\n",
              " 'c',\n",
              " 'quit',\n",
              " 'welcom',\n",
              " 'enough',\n",
              " 'yet',\n",
              " 'current',\n",
              " 'freedom',\n",
              " 'matter',\n",
              " 'ye',\n",
              " 'correct',\n",
              " 'thought',\n",
              " 'probabl',\n",
              " 'explain',\n",
              " 'exampl',\n",
              " 'term',\n",
              " 'notic',\n",
              " 'power',\n",
              " 'im',\n",
              " 'languag',\n",
              " 'e',\n",
              " 'lot',\n",
              " 'import',\n",
              " 'great',\n",
              " 'appear',\n",
              " 'american',\n",
              " 'lie',\n",
              " 'move',\n",
              " 'websit',\n",
              " 'number',\n",
              " 'complet',\n",
              " 'mother',\n",
              " 'mayb',\n",
              " 'review',\n",
              " 'reliabl',\n",
              " 'english',\n",
              " 'org',\n",
              " 'exist',\n",
              " 'found',\n",
              " 'huge',\n",
              " 'address',\n",
              " 'text',\n",
              " 'happen',\n",
              " 'idea',\n",
              " 'statement',\n",
              " 'dog',\n",
              " 'twat',\n",
              " 'play',\n",
              " 'fucksex',\n",
              " 'yourselfgo',\n",
              " 'instead',\n",
              " 'abus',\n",
              " 'titl',\n",
              " 'hello',\n",
              " 'www',\n",
              " 'commun',\n",
              " 'simpli',\n",
              " 'clearli',\n",
              " 'rather',\n",
              " 'homo',\n",
              " 'least',\n",
              " 'either',\n",
              " 'kind',\n",
              " 'head',\n",
              " 'encyclopedia',\n",
              " 'copyright',\n",
              " 'pictur',\n",
              " 'non',\n",
              " 'style',\n",
              " 'accept',\n",
              " 'retard',\n",
              " 'templat',\n",
              " 'game',\n",
              " 'nation',\n",
              " 'far',\n",
              " 'etc',\n",
              " 'hitler',\n",
              " 'evid',\n",
              " 'alway',\n",
              " 'true',\n",
              " 'cite',\n",
              " 'super',\n",
              " 'bit',\n",
              " 'nice',\n",
              " 'accus',\n",
              " 'end',\n",
              " 'version',\n",
              " 'materi',\n",
              " 'fair',\n",
              " 'group',\n",
              " 'allow',\n",
              " 'quot',\n",
              " 'possibl',\n",
              " 'act',\n",
              " 'recent',\n",
              " 'countri',\n",
              " 'school',\n",
              " 'friend',\n",
              " 'base',\n",
              " 'mind',\n",
              " 'en',\n",
              " 'written',\n",
              " 'topic',\n",
              " 'relat',\n",
              " 'clear',\n",
              " 'nigga',\n",
              " 'watch',\n",
              " 'polit',\n",
              " 'away',\n",
              " 'small',\n",
              " 'sign',\n",
              " 'pro',\n",
              " 'nazi',\n",
              " 'left',\n",
              " 'given',\n",
              " 'poop',\n",
              " 'research',\n",
              " 'everyon',\n",
              " 'self',\n",
              " 'action',\n",
              " 'improv',\n",
              " 'truth',\n",
              " 'sever',\n",
              " 'buttseck',\n",
              " 'p',\n",
              " 'consensu',\n",
              " 'ok',\n",
              " 'unblock',\n",
              " 'second',\n",
              " 'wrote',\n",
              " 'mothjer',\n",
              " 'answer',\n",
              " 'violat',\n",
              " 'pussi',\n",
              " 'noob',\n",
              " 'caus',\n",
              " 'less',\n",
              " 'author',\n",
              " 'stuff',\n",
              " 'fool',\n",
              " 'pov',\n",
              " 'learn',\n",
              " 'bush',\n",
              " 'b',\n",
              " 'speedi',\n",
              " 'wish',\n",
              " 'fggt',\n",
              " 'anyway',\n",
              " 'whole',\n",
              " 'top',\n",
              " 'stay',\n",
              " 'citat',\n",
              " 'cannot',\n",
              " 'line',\n",
              " 'hard',\n",
              " 'posit',\n",
              " 'fire',\n",
              " 'dumb',\n",
              " 'public',\n",
              " 'guidelin',\n",
              " 'project',\n",
              " 'alon',\n",
              " 'whether',\n",
              " 'lol',\n",
              " 'seen',\n",
              " 'sexual',\n",
              " 'specif',\n",
              " 'respons',\n",
              " 'full',\n",
              " 'respect',\n",
              " 'fan',\n",
              " 'definit',\n",
              " 'crap',\n",
              " 'redirect',\n",
              " 'protect',\n",
              " 'perhap',\n",
              " 'baster',\n",
              " 'anti',\n",
              " 'concern',\n",
              " 'r',\n",
              " 'decid',\n",
              " 'racist',\n",
              " 'media',\n",
              " 'troll',\n",
              " 'cours',\n",
              " 'lick',\n",
              " 'date',\n",
              " 'critic',\n",
              " 'guess',\n",
              " 'info',\n",
              " 'deal',\n",
              " 'whatev',\n",
              " 'three',\n",
              " 'comput',\n",
              " 'assad',\n",
              " 'yeah',\n",
              " 'neutral',\n",
              " 'argument',\n",
              " 'repli',\n",
              " 'anal',\n",
              " 'hand',\n",
              " 'common',\n",
              " 'sens',\n",
              " 'past',\n",
              " 'speak',\n",
              " 'involv',\n",
              " 'ha',\n",
              " 'categori',\n",
              " 'serious',\n",
              " 'file',\n",
              " 'entir',\n",
              " 'week',\n",
              " 'pathet',\n",
              " 'propos',\n",
              " 'attempt',\n",
              " 'obvious',\n",
              " 'present',\n",
              " 'happi',\n",
              " 'band',\n",
              " 'side',\n",
              " 'mexican',\n",
              " 'black',\n",
              " 'addit',\n",
              " 'member',\n",
              " 'next',\n",
              " 'news',\n",
              " 'n',\n",
              " 'internet',\n",
              " 'becom',\n",
              " 'fals',\n",
              " 'sentenc',\n",
              " 'everyth',\n",
              " 'white',\n",
              " 'contrib',\n",
              " 'major',\n",
              " 'face',\n",
              " 'lead',\n",
              " 'heil',\n",
              " 'order',\n",
              " 'type',\n",
              " 'j',\n",
              " 'relev',\n",
              " 'appreci',\n",
              " 'known',\n",
              " 'main',\n",
              " 'boob',\n",
              " 'jewish',\n",
              " 'open',\n",
              " 'run',\n",
              " 'disput',\n",
              " 'fix',\n",
              " 'univers',\n",
              " 'ur',\n",
              " 'parti',\n",
              " 'describ',\n",
              " 'upload',\n",
              " 'job',\n",
              " 'th',\n",
              " 'requir',\n",
              " 'hour',\n",
              " 'enjoy',\n",
              " 'result',\n",
              " 'human',\n",
              " 'prove',\n",
              " 'offici',\n",
              " 'close',\n",
              " 'chicken',\n",
              " 'final',\n",
              " 'wonder',\n",
              " 'rememb',\n",
              " 'total',\n",
              " 'appropri',\n",
              " 'object',\n",
              " 'music',\n",
              " 'taken',\n",
              " 'fart',\n",
              " 'wikipedian',\n",
              " 'useless',\n",
              " 'eye',\n",
              " 'summari',\n",
              " 'fuckin',\n",
              " 'faith',\n",
              " 'bot',\n",
              " 'hanib',\n",
              " 'especi',\n",
              " 'accord',\n",
              " 'publish',\n",
              " 'detail',\n",
              " 'million',\n",
              " 'nomin',\n",
              " 'high',\n",
              " 'cool',\n",
              " 'compani',\n",
              " 'insult',\n",
              " 'offfuck',\n",
              " 'pretti',\n",
              " 'minor',\n",
              " 'unit',\n",
              " 'criteria',\n",
              " 'month',\n",
              " 'featur',\n",
              " 'death',\n",
              " 'told',\n",
              " 'singl',\n",
              " 'fail',\n",
              " 'sort',\n",
              " 'standard',\n",
              " 'test',\n",
              " 'christian',\n",
              " 'later',\n",
              " 'verifi',\n",
              " 'proof',\n",
              " 'cover',\n",
              " 'sound',\n",
              " 'today',\n",
              " 'law',\n",
              " 'busi',\n",
              " 'entri',\n",
              " 'came',\n",
              " 'forc',\n",
              " 'bollock',\n",
              " 'china',\n",
              " 'promot',\n",
              " 'form',\n",
              " 'g',\n",
              " 'famili',\n",
              " 'civil',\n",
              " 'certainli',\n",
              " 'rice',\n",
              " 'googl',\n",
              " 'due',\n",
              " 'controversi',\n",
              " 'experi',\n",
              " 'system',\n",
              " 'per',\n",
              " 'boy',\n",
              " 'along',\n",
              " 'stand',\n",
              " 'sock',\n",
              " 'sexsex',\n",
              " 'k',\n",
              " 'assum',\n",
              " 'ago',\n",
              " 'process',\n",
              " 'indic',\n",
              " 'abl',\n",
              " 'kid',\n",
              " 'repeat',\n",
              " 'govern',\n",
              " 'citi',\n",
              " 'search',\n",
              " 'stori',\n",
              " 'harass',\n",
              " 'whore',\n",
              " 'threat',\n",
              " 'nonsens',\n",
              " 'german',\n",
              " 'fun',\n",
              " 'charact',\n",
              " 'meet',\n",
              " 'disrupt',\n",
              " 'although',\n",
              " 'respond',\n",
              " 'knowledg',\n",
              " 'wait',\n",
              " 'turn',\n",
              " 'film',\n",
              " 'piss',\n",
              " 'color',\n",
              " 'area',\n",
              " 'vote',\n",
              " 'lawdi',\n",
              " 'futur',\n",
              " 'therefor',\n",
              " 'theori',\n",
              " 'except',\n",
              " 'unless',\n",
              " 'notrhbysouthbanof',\n",
              " 'bother',\n",
              " 'wast',\n",
              " 'wale',\n",
              " 'suppos',\n",
              " 'exactli',\n",
              " 'copi',\n",
              " 'gonna',\n",
              " 'contact',\n",
              " 'joke',\n",
              " 'x',\n",
              " 'expect',\n",
              " 'control',\n",
              " 'basic',\n",
              " 'took',\n",
              " 'realiz',\n",
              " 'jpg',\n",
              " 'soon',\n",
              " 'smell',\n",
              " 'reader',\n",
              " 'john',\n",
              " 'bias',\n",
              " 'releas',\n",
              " 'fine',\n",
              " 'class',\n",
              " 'song',\n",
              " 'son',\n",
              " 'british',\n",
              " 'paragraph',\n",
              " 'record',\n",
              " 'mistak',\n",
              " 'mr',\n",
              " 'four',\n",
              " 'apolog',\n",
              " 'valid',\n",
              " 'king',\n",
              " 'activ',\n",
              " 'spanish',\n",
              " 'organ',\n",
              " 'cultur',\n",
              " 'terrorist',\n",
              " 'oxymoron',\n",
              " 'direct',\n",
              " 'goe',\n",
              " 'within',\n",
              " 'set',\n",
              " 'construct',\n",
              " 'appar',\n",
              " 'prick',\n",
              " 'web',\n",
              " 'usernam',\n",
              " 'similar',\n",
              " 'church',\n",
              " 'replac',\n",
              " 'figur',\n",
              " 'attent',\n",
              " 'lack',\n",
              " 'deserv',\n",
              " 'bia',\n",
              " 'went',\n",
              " 'miss',\n",
              " 'absolut',\n",
              " 'sometim',\n",
              " 'criminalwar',\n",
              " 'product',\n",
              " 'bunkstev',\n",
              " 'background',\n",
              " 'develop',\n",
              " 'dead',\n",
              " 'bring',\n",
              " 'studi',\n",
              " 'dude',\n",
              " 'murder',\n",
              " 'hous',\n",
              " 'cant',\n",
              " 'video',\n",
              " 'updat',\n",
              " 'sick',\n",
              " 'saw',\n",
              " 'archiv',\n",
              " 'spell',\n",
              " 'romney',\n",
              " 'photo',\n",
              " 'chester',\n",
              " 'disagre',\n",
              " 'almost',\n",
              " 'email',\n",
              " 'bloodi',\n",
              " 'avoid',\n",
              " 'men',\n",
              " 'event',\n",
              " 'log',\n",
              " 'bum',\n",
              " 'larg',\n",
              " 'dirti',\n",
              " 'special',\n",
              " 'obviou',\n",
              " 'mitt',\n",
              " 'listen',\n",
              " 'fight',\n",
              " 'uk',\n",
              " 'muslim',\n",
              " 'inde',\n",
              " 'accur',\n",
              " 'purpos',\n",
              " 'motherfuck',\n",
              " 'marcolfuck',\n",
              " 'censor',\n",
              " 'bunch',\n",
              " 'argu',\n",
              " 'togeth',\n",
              " 'debat',\n",
              " 'children',\n",
              " 'wikiproject',\n",
              " 'seriou',\n",
              " 'otherwis',\n",
              " 'npov',\n",
              " 'homosexu',\n",
              " 'remain',\n",
              " 'level',\n",
              " 'histor',\n",
              " 'explan',\n",
              " 'destroy',\n",
              " 'confirm',\n",
              " 'sad',\n",
              " 'individu',\n",
              " 'ars',\n",
              " 'usual',\n",
              " 'longer',\n",
              " 'jerk',\n",
              " 'simpl',\n",
              " 'that',\n",
              " 'dear',\n",
              " 'girl',\n",
              " 'conflict',\n",
              " 'women',\n",
              " 'confus',\n",
              " 'contain',\n",
              " 'ya',\n",
              " 'v',\n",
              " 'spam',\n",
              " 'tommi',\n",
              " 'wow',\n",
              " 'robert',\n",
              " 'natur',\n",
              " 'lost',\n",
              " 'homeland',\n",
              " 'ahead',\n",
              " 'produc',\n",
              " 'preced',\n",
              " 'poor',\n",
              " 'heard',\n",
              " 'certain',\n",
              " 'stick',\n",
              " 'statu',\n",
              " 'rest',\n",
              " 'often',\n",
              " 'l',\n",
              " 'effect',\n",
              " 'admit',\n",
              " 'home',\n",
              " 'georg',\n",
              " 'chees',\n",
              " 'threaten',\n",
              " 'translat',\n",
              " 'award',\n",
              " 'atheist',\n",
              " 'sandbox',\n",
              " 'ridicul',\n",
              " 'document',\n",
              " 'liar',\n",
              " 'nobodi',\n",
              " 'march',\n",
              " 'awar',\n",
              " 'anim',\n",
              " 'vomit',\n",
              " 'merg',\n",
              " 'fack',\n",
              " 'sockpuppet',\n",
              " 'mark',\n",
              " 'liber',\n",
              " 'cougar',\n",
              " 'behind',\n",
              " 'jesu',\n",
              " 'intern',\n",
              " 'logic',\n",
              " 'intent',\n",
              " 'contributor',\n",
              " 'cheer',\n",
              " 'outsid',\n",
              " 'diff',\n",
              " 'centuri',\n",
              " 'gave',\n",
              " 'share',\n",
              " 'securityfuck',\n",
              " 'particular',\n",
              " 'blank',\n",
              " 'behavior',\n",
              " 'ugli',\n",
              " 'greek',\n",
              " 'funni',\n",
              " 'cuntbag',\n",
              " 'unsign',\n",
              " 'okay',\n",
              " 'extern',\n",
              " 'bulli',\n",
              " 'born',\n",
              " 'third',\n",
              " 'multipl',\n",
              " 'practic',\n",
              " 'album',\n",
              " 'situat',\n",
              " 'islam',\n",
              " 'child',\n",
              " 'arab',\n",
              " 'w',\n",
              " 'none',\n",
              " 'begin',\n",
              " 'upon',\n",
              " 'send',\n",
              " 'movi',\n",
              " 'gone',\n",
              " 'five',\n",
              " 'variou',\n",
              " 'space',\n",
              " 'ruin',\n",
              " 'youbollock',\n",
              " 'egg',\n",
              " 'doubt',\n",
              " 'break',\n",
              " 'appli',\n",
              " 'prefer',\n",
              " 'de',\n",
              " 'push',\n",
              " 'mouth',\n",
              " 'cut',\n",
              " 'team',\n",
              " 'mere',\n",
              " 'anthoni',\n",
              " 'shall',\n",
              " 'hit',\n",
              " 'belong',\n",
              " 'veggietal',\n",
              " 'toward',\n",
              " 'oppos',\n",
              " 'edgar',\n",
              " 'money',\n",
              " 'jim',\n",
              " 'insert',\n",
              " 'effort',\n",
              " 'ah',\n",
              " 'servic',\n",
              " 'seri',\n",
              " 'drink',\n",
              " 'defend',\n",
              " 'short',\n",
              " 'receiv',\n",
              " 'mine',\n",
              " 'hear',\n",
              " 'half',\n",
              " 'descript',\n",
              " 'colleg',\n",
              " 'express',\n",
              " 'coupl',\n",
              " 'ancestryfuck',\n",
              " 'religion',\n",
              " 'co',\n",
              " 'bodi',\n",
              " 'age',\n",
              " 'sb',\n",
              " 'proper',\n",
              " 'extrem',\n",
              " 'ga',\n",
              " 'tv',\n",
              " 'dare',\n",
              " 'america',\n",
              " 'tabl',\n",
              " 'sit',\n",
              " 'vagina',\n",
              " 'step',\n",
              " 'signific',\n",
              " 'previou',\n",
              " 'indian',\n",
              " 'error',\n",
              " 'data',\n",
              " 'spend',\n",
              " 'propaganda',\n",
              " 'biographi',\n",
              " 'anymor',\n",
              " 'restor',\n",
              " 'parent',\n",
              " 'minut',\n",
              " 'educ',\n",
              " 'bag',\n",
              " 'serv',\n",
              " 'particip',\n",
              " 'ill',\n",
              " 'count',\n",
              " 'locat',\n",
              " 'delanoy',\n",
              " 'decis',\n",
              " 'context',\n",
              " 'russian',\n",
              " 'offens',\n",
              " 'nhrh',\n",
              " 'mess',\n",
              " 'ethnic',\n",
              " 'chanc',\n",
              " 'repres',\n",
              " 'puppet',\n",
              " 'licker',\n",
              " 'grow',\n",
              " 'credit',\n",
              " 'august',\n",
              " 'refus',\n",
              " 'red',\n",
              " 'episod',\n",
              " 'student',\n",
              " 'rude',\n",
              " 'mum',\n",
              " 'clean',\n",
              " 'separ',\n",
              " 'san',\n",
              " 'referenc',\n",
              " 'brain',\n",
              " 'forev',\n",
              " 'douch',\n",
              " 'altern',\n",
              " 'win',\n",
              " 'return',\n",
              " 'popul',\n",
              " 'worth',\n",
              " 'format',\n",
              " 'fit',\n",
              " 'cathol',\n",
              " 'annoy',\n",
              " 'race',\n",
              " 'period',\n",
              " 'hold',\n",
              " 'automat',\n",
              " 'remark',\n",
              " 'pick',\n",
              " 'mongo',\n",
              " 'html',\n",
              " 'excus',\n",
              " 'drop',\n",
              " 'ullmann',\n",
              " 'thu',\n",
              " 'licens',\n",
              " 'censorship',\n",
              " 'bradburi',\n",
              " 'avail',\n",
              " 'shitfuck',\n",
              " 'qualiti',\n",
              " 'polic',\n",
              " 'offer',\n",
              " 'forum',\n",
              " 'factual',\n",
              " 'militari',\n",
              " 'legal',\n",
              " 'er',\n",
              " 'box',\n",
              " 'pass',\n",
              " 'valu',\n",
              " 'treat',\n",
              " 'evil',\n",
              " 'click',\n",
              " 'onlin',\n",
              " 'join',\n",
              " 'social',\n",
              " 'shot',\n",
              " 'scienc',\n",
              " 'save',\n",
              " 'reach',\n",
              " 'limit',\n",
              " 'centraliststupid',\n",
              " 'assert',\n",
              " 'afd',\n",
              " 'six',\n",
              " 'interpret',\n",
              " 'access',\n",
              " 'troubl',\n",
              " ...]"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# To show example of the vectorization\n",
        "# Numeric reprentation of the word\n",
        "vectorizer ('Final project , lets make it fun')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3BTnfBmoS06M",
        "outputId": "89b896b3-62f6-4616-ca5a-c2304d1ec4f1"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1800,), dtype=int64, numpy=array([508, 401,   1, ...,   0,   0,   0])>"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Tokenizing every single word\n",
        "vectorized_text = vectorizer(X.values)"
      ],
      "metadata": {
        "id": "S0UdnsWzS05I"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Total of 31220 sentences vectorize,capped at 1800 words\n",
        "vectorized_text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QuhePzmwS01v",
        "outputId": "1ef2afd1-ca6e-4d04-fe0b-ef32cdb4345c"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(31220, 1800), dtype=int64, numpy=\n",
              "array([[    2, 10801,     8, ...,     0,     0,     0],\n",
              "       [   22,     8,  2072, ...,     0,     0,     0],\n",
              "       [  253,   156,  1540, ...,     0,     0,     0],\n",
              "       ...,\n",
              "       [  244,   294,   244, ...,     0,     0,     0],\n",
              "       [    2,  1192,     7, ...,     0,     0,     0],\n",
              "       [ 1514, 29004,    15, ...,     0,     0,     0]])>"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Data pipeline steps\n",
        "#MCSHBAP - map, chache, shuffle, batch, prefetch to build data pipeline from_tensor_slices\n",
        "dataset = tf.data.Dataset.from_tensor_slices((vectorized_text, y))\n",
        "dataset = dataset.cache()\n",
        "dataset = dataset.shuffle(160000)\n",
        "dataset = dataset.batch(16)\n",
        "dataset = dataset.prefetch(8)# helps prevent bottlenecks"
      ],
      "metadata": {
        "id": "bQV1szkJTOLL"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Splitting Data Set for training, validation and testing"
      ],
      "metadata": {
        "id": "8EVYj2HpPmb5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 70% training data, 20% validation and 10 % test\n",
        "train = dataset.take(int(len(dataset)*.7)) # specify the num of batches\n",
        "val = dataset.skip(int(len(dataset)*.7)).take(int(len(dataset)*.2)) # will skip the 1st 70%\n",
        "test = dataset.skip(int(len(dataset)*.9)).take(int(len(dataset)*.1)) # will skip the 1st 9\n"
      ],
      "metadata": {
        "id": "JZa4EmstTOEJ"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Total data divide by 16 batch will get 9974 batches in total\n",
        "len(dataset)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wtl3Sp-mTN_g",
        "outputId": "3eec5308-1388-45f0-948f-3e9806ccfe09"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1952"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# total train batches\n",
        "len(train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XUAsqEsWVKbh",
        "outputId": "0c9d4a92-8770-4c75-85d3-ad708b616869"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1366"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# total validation batches\n",
        "len (val)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N1v32j37VKWz",
        "outputId": "e994ebab-0128-4b5b-f5cd-4a55c04184eb"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "390"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# total of test batches\n",
        "len(test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p_VCDdcAVajB",
        "outputId": "863190db-d30a-4775-aa2d-4c7cbf01ee20"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "195"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For Deep Learning Machine Model : It will pass thru a batch into into forward pass ,it will them\n",
        "go and do a backward pass, and it will update the gradients and will follow with the next batch\n"
      ],
      "metadata": {
        "id": "PR25FASBVjIB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create a generator and .next get the next batch\n",
        "# everytime we run this code ,it will fetch the next batch\n",
        "train_generator = train.as_numpy_iterator()\n",
        "train_generator.next()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xhQbSAQTVaYz",
        "outputId": "270b6938-51c8-42bd-af0a-3763263d6d77"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([[   86,   219,   219, ...,     0,     0,     0],\n",
              "        [ 1246,    20,    58, ...,     0,     0,     0],\n",
              "        [    2,   779,    14, ...,     0,     0,     0],\n",
              "        ...,\n",
              "        [21343,    44,     9, ...,     0,     0,     0],\n",
              "        [  222,   236,    13, ...,     0,     0,     0],\n",
              "        [   62,  3188,  3967, ...,     0,     0,     0]]),\n",
              " array([[1, 0, 0, 0, 1, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [1, 1, 1, 0, 1, 0],\n",
              "        [1, 0, 1, 0, 1, 0],\n",
              "        [1, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [1, 0, 0, 0, 1, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [1, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [1, 0, 1, 0, 1, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [1, 1, 1, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0]]))"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating Sequential Deep Learning Model"
      ],
      "metadata": {
        "id": "D9YtHDVbVtkw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download Sequential model\n",
        "from tensorflow.keras.models import Sequential\n",
        "# Layers to build deep neural networks\n",
        "from tensorflow.keras.layers import LSTM, Dropout, Bidirectional, Dense, Embedding"
      ],
      "metadata": {
        "id": "R4ivHwSmVtEO"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "# Create the embedding layer\n",
        "model.add(Embedding(max_word+1, 32))\n",
        "# Bidirectional LSTM Layer\n",
        "model.add(Bidirectional(LSTM(32, activation='tanh')))\n",
        "# Feature extractor Fully connected layers\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dense(256, activation='relu'))\n",
        "model.add(Dense(128, activation='relu'))\n",
        "# Final layer\n",
        "model.add(Dense(6, activation='sigmoid'))"
      ],
      "metadata": {
        "id": "xZxtJm-5VtC_"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer=keras.optimizers.Adam(learning_rate=0.003)"
      ],
      "metadata": {
        "id": "btYqSl5_FCgm"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss='BinaryCrossentropy', optimizer= optimizer , metrics = ['accuracy'])"
      ],
      "metadata": {
        "id": "45_fRTMeVs3V"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8-848_RkVs2b",
        "outputId": "f3095546-f2fa-403d-9945-76abc3b954d5"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, None, 32)          6400032   \n",
            "                                                                 \n",
            " bidirectional (Bidirectiona  (None, 64)               16640     \n",
            " l)                                                              \n",
            "                                                                 \n",
            " dense (Dense)               (None, 128)               8320      \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 256)               33024     \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 128)               32896     \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 6)                 774       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 6,491,686\n",
            "Trainable params: 6,491,686\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Had to limit the epochs as GPU is not available for better training of model.\n",
        "# We can achive a better accuracy level by increasing the epochs number.\n",
        "history = model.fit(train, epochs=2, validation_data=val)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FzEh6U4pVsy_",
        "outputId": "7946ab8c-ad33-4d9d-c0b1-1629b94a463a"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2\n",
            "1366/1366 [==============================] - 3119s 2s/step - loss: 0.1486 - accuracy: 0.9655 - val_loss: 0.1231 - val_accuracy: 0.9736\n",
            "Epoch 2/2\n",
            "1366/1366 [==============================] - 3000s 2s/step - loss: 0.1302 - accuracy: 0.9512 - val_loss: 0.1127 - val_accuracy: 0.9707\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing out the model with new inputs:"
      ],
      "metadata": {
        "id": "efrhFb6Q6kDv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing out the model. Test 1:\n",
        "input_text = vectorizer('You freaking suck! I am going to hit you')"
      ],
      "metadata": {
        "id": "kFJPQ8L0hlyO"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns[1:]"
      ],
      "metadata": {
        "id": "XjPB9gExhlu2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fe46ad7e-3eba-49da-b4b0-b7a264cfb14f"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['toxic', 'severe_toxic', 'obscene', 'threat', 'insult',\n",
              "       'identity_hate'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result = model.predict(np.expand_dims(input_text , 0 ))\n"
      ],
      "metadata": {
        "id": "o6ALg9x3h3Ck",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ca27bbf-b6ed-4cf8-9007-4b909bd83ef9"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 3s 3s/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(result > 0.5).astype(int)\n"
      ],
      "metadata": {
        "id": "-q4vV4_Gh27v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc100422-7267-47e7-a95b-905bcdc6779d"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1, 0, 1, 0, 1, 0]])"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test 2:\n",
        "input_text = vectorizer ('Congratulations! That was a great performance')\n",
        "\n",
        "result = model.predict(np.expand_dims(input_text , 0 ))\n",
        "(result > 0.5).astype(int)"
      ],
      "metadata": {
        "id": "Qtbj8gkWjJ5a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "493f0a0f-c52a-4090-cc42-e6f1791dc94e"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 330ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 0, 0, 0, 0, 0]])"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test 3:\n",
        "# Its purely for demonstration purposes,to test if  the model able to recognize 'identity_hate' (sorry)\n",
        "input_text = vectorizer (' back off you nigger')\n",
        "\n",
        "result = model.predict(np.expand_dims(input_text , 0 ))\n",
        "(result > 0.5).astype(int)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QrgqKBklfRVr",
        "outputId": "fa8cb72f-4198-4fb1-a95d-963a5d283afd"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 162ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1, 0, 1, 0, 1, 1]])"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Download Library to save the model\n",
        "from tensorflow.keras.models import load_model"
      ],
      "metadata": {
        "id": "fKaKR2ofbZxY"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the model file so it can be just tested in the future without the need of running it again\n",
        "model.save('/content/drive/MyDrive/Trained_Model/toxic_trained.h5')"
      ],
      "metadata": {
        "id": "CkwZAFEEbZVA"
      },
      "execution_count": 53,
      "outputs": []
    }
  ]
}